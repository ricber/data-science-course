{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "<tr>\n",
    "    <td><h1 style=\"text-align: left; font-size:300%;\">\n",
    "        Classification Hands On\n",
    "    </h1></td>\n",
    "    <td width=\"30%\">\n",
    "    <div style=\"text-align: right\">\n",
    "    <b> Practical Data Science Lessons</b><br><br>\n",
    "    <b> Riccardo Bertoglio</b><br>\n",
    "    <a href=\"mailto:riccardo.bertoglio@polimi.it\">riccardo.bertoglio@polimi.it</a><br>\n",
    "    </div>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Exercise source: Harvard 2021 CS109-A: Introduction to Data Science](https://harvard-iacs.github.io/2021-CS109A/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NEW packages\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Visualization packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset\n",
    "\n",
    "\n",
    "- Load and get to know a new dataset\n",
    "- Select our predictors and response variable\n",
    "- Generate a train-test-split\n",
    "- Scale our data\n",
    "- Perform logistic regression and investigate our results\n",
    "\n",
    "![wine](fig/wine.jpg)\n",
    "\n",
    "*Image source: https://en.wikipedia.org/wiki/Wine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_wine = datasets.load_wine()\n",
    "# print(dataset_wine.keys())\n",
    "# print()\n",
    "print(dataset_wine['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "(178, 1)\n"
     ]
    }
   ],
   "source": [
    "X_wine = pd.DataFrame(data=dataset_wine.data, columns=dataset_wine.feature_names)\n",
    "y_wine = pd.DataFrame(data=dataset_wine.target, columns=['class'])\n",
    "print(X_wine.shape)\n",
    "print(y_wine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>0.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "            class  \n",
       "count  178.000000  \n",
       "mean     0.938202  \n",
       "std      0.775035  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      2.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_wine = pd.concat([X_wine, y_wine], axis=1)\n",
    "full_df_wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pairplot in the cell below to see a scatter-matrix of all variables contained in the wine dataset. Use the `hue` parameter to color the points based on their class.\n",
    "\n",
    "**Please note:** The plotting code will take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "___ # Your code here\n",
    "plt.suptitle('Wine dataset features, colored by cultivator class', fontsize=30, y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The wine dataset provides many variables to use in our model.\n",
    "\n",
    "- **Because we have limited time for this exercise, we will pretend we have already conducted an Exploratory Data Analysis (EDA) on our dataset and have chosen to focus solely on the predictors `alcohol` and `flavanoids`.**\n",
    "\n",
    "- Select the featutes to subset our full wine dataframe and perform a train-test-split with 0.2 test size and stratified shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ____ # Your code here\n",
    "\n",
    "full_df_wine_train, full_df_wine_test, = ____ # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's take a quick look at our training data to get a sense for how `alcohol` and `flavanoids` relate to one another, as well as to our cultivator `class` response variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=full_df_wine_train,\n",
    "    x='alcohol', y='flavanoids',\n",
    "    hue='class', palette = sns.color_palette(\"tab10\")[:3])\n",
    "plt.title(\n",
    "    'Wine training observations,\\ncolored by cultivator class',\n",
    "    fontsize=14\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice about the scale of our two predictor variables?**\n",
    "\n",
    "- It looks like the mean is shifted, but the total range of values are similar.\n",
    "- Let's standardize our predictors to center our points and to ensure that both variables are commonly scaled. Instantiate a `StandardScaler` object a fit it on the train data. Then transform the predictors. \n",
    "- **IMPORTANT:** Remember to fit your scaler ONLY on your training data and to transform both train and test from that training fit. NEVER fit your scaler on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## SUBSET OUR X AND y DATAFRAMES ##\n",
    "###################################\n",
    "\n",
    "X_wine_train, y_wine_train = ____ # Your code here\n",
    "X_wine_test, y_wine_test = ____ # Your code here\n",
    "\n",
    "# print(\n",
    "#     \"Summary statistics for our training predictors BEFORE standardizing:\\n\\n\"\n",
    "#     \"{}\\n\".format(X_wine_train.describe())\n",
    "# )\n",
    "\n",
    "##########################\n",
    "## SCALE THE PREDICTORS ##\n",
    "##########################\n",
    "\n",
    "# Be certain to ONLY EVER fit your scaler on X train (NEVER fit it on test)\n",
    "scaler = ____ # Your code here\n",
    "\n",
    "# Use your train-fitted scaler to transform both X train and X test\n",
    "X_wine_train[predictors] = ____ # Your code here\n",
    "X_wine_test[predictors] = ____ # Your code here\n",
    "\n",
    "# print(\n",
    "#     \"Summary statistics for our training predictors AFTER standardizing:\\n\\n\"\n",
    "#     \"{}\".format(X_wine_train.describe())\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "1. Fit a logistic regression model (name it `model1_wine`) without regularization (i.e. `penalty='none'`) to predict the `class` of each wine using our scaled predictors `alcohol` and `flavanoids`\n",
    "2. Report on the training and test accuracy of our fitted model\n",
    "3. Show that for each training prediction, the probability of each predicted class sums to one. (**HINT:** You can use the `predict_proba` method to generate your predicted probabilities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Train model\n",
    "model1_wine = ___\n",
    "\n",
    "# Score model\n",
    "train_score = ___\n",
    "test_score = ___\n",
    "\n",
    "# Print scores\n",
    "___\n",
    "\n",
    "# Predict probabilities for our training data\n",
    "y_proba_train = ___\n",
    "\n",
    "\n",
    "# Check shape of our predictions to show that we have 3 probabilities predicted\n",
    "# for each observation (i.e. predicted probabilities for each of our 3 classes)\n",
    "___\n",
    "\n",
    "# Sum all 3 classes at each observation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the decision boundaries\n",
    "Using the `plot_wine_2d_boundaries` function provided below, you are going to plot the TRAINING data decision boundaries for `model1_wine` from the exercise above.\n",
    "\n",
    "**To do this, you will need to:**\n",
    "\n",
    "1. Define an appropriate `xx_1_wine` and `xx_2_wine` for input into this function using `np.meshgrid`\n",
    "2. And call `plot_wine_2d_boundaries` specifying the appropriate input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you the plotting function here\n",
    "def plot_wine_2d_boundaries(X_data, y_data, predictors, model, xx_1, xx_2):\n",
    "    \"\"\"Plots 2-dimensional decision boundaries for a fitted sklearn model\n",
    "    \n",
    "    :param X_data: pd.DataFrame object containing your predictor variables\n",
    "    :param y_data: pd.Series object containing your response variable\n",
    "    :param predictors: list of predictor names corresponding with X_data columns\n",
    "    :param model: sklearn fitted model object\n",
    "    :param xx_1: np.array object of 2-dimensions, generated using np.meshgrid\n",
    "    :param xx_2: np.array object of 2-dimensions, generated using np.meshgrid\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_points(ax):\n",
    "        for i, y_class in enumerate(set(y_data.values.flatten())):\n",
    "            index = (y_data == y_class).values\n",
    "            ax.scatter(\n",
    "                X_data[index][predictors[0]],\n",
    "                X_data[index][predictors[1]],\n",
    "                c=colors[i],\n",
    "                marker=markers[i],\n",
    "                s=65, \n",
    "                edgecolor='w',\n",
    "                label=\"class {}\".format(i),\n",
    "            )\n",
    "\n",
    "    # Plotting decision regions\n",
    "    f, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "    X_mesh = np.stack((xx_1.ravel(), xx_2.ravel()), axis=1)\n",
    "\n",
    "    Z = model.predict(X_mesh)\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "    ax.contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "\n",
    "    plot_points(ax)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "    ax.set_title(\n",
    "        'Wine cultivator class prediction\\ndecision boundaries',\n",
    "        fontsize=16\n",
    "    )\n",
    "    ax.set_xlabel(predictors[0], fontsize=12)\n",
    "    ax.set_ylabel(predictors[1], fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "#### Enter your code below\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n",
    "# Using an approach similar to what was used in the Iris example\n",
    "# we can identify appropriate boundaries for our meshgrid by\n",
    "# referencing the actual wine data\n",
    "\n",
    "x_1_wine = ___\n",
    "x_2_wine = ___\n",
    "\n",
    "\n",
    "# Then we use np.arange to generate our interval arrays\n",
    "# and np.meshgrid to generate our actual grids\n",
    "\n",
    "xx_1_wine, xx_2_wine = ___\n",
    "\n",
    "# Now we have everything we need to generate our plot\n",
    "\n",
    "plot_wine_2d_boundaries(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Tune and fit a Lasso regularized model using cross-validation.**\n",
    "\n",
    "\n",
    "- Perform Lasso regularized logistic regression and choose an appropriate `C` by using cross-validation\n",
    "\n",
    "\n",
    "- Confirm whether any predictors are identified as unimportant (i.e. $w_i=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**There are a number of different tools built into `sklearn` that help you to perform cross-validation.**\n",
    "\n",
    "- A lot of examples in this class so far have used `sklearn.model_selection.cross_validate` as the primary means for peforming cross-validation.\n",
    "\n",
    "\n",
    "- **BUT WAIT!!!** As it turns out, `sklearn` provides a very useful tool for performing logistic regression cross-validation across a range of regularization hyperparameters.\n",
    "\n",
    "**For this problem, you should use the `sklearn.linear_model.LogisticRegressionCV`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "1. Add a new predictor to your `X` data measuring the interaction between `alcohol` and `flavanoid` (make certain to do this for both your training and TEST `X` dataframes).\n",
    "\n",
    "    \n",
    "2. Please review the documentation to learn how to fit and use the `LogisticRegressionCV` model object: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "\n",
    "    \n",
    "3. Using `LogisticRegressionCV` to fit your model, perfrom cross-validation with `3` k-folds, Lasso-like regularization, and the following list of regularization parameters `[1e-4,1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3,1e4]`.\n",
    "\n",
    "    \n",
    "4. Print (1) the regularization parameter chosen by the model, (2) your train and test accuracies, and (3) your model coefficients (including the intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here.\n",
    "\n",
    "# create interaction term for both train and test sets\n",
    "___\n",
    "\n",
    "# View the resulting dataframe\n",
    "display(X_wine_train.head())\n",
    "\n",
    "print()\n",
    "\n",
    "# Fit cv logistic regression model to predictors (including interaction term)\n",
    "\n",
    "model2_wine = LogisticRegressionCV(\n",
    "   ___\n",
    ").fit(X_wine_train, y_wine_train)\n",
    "\n",
    "model2_score_train = ___\n",
    "model2_score_test = ___\n",
    "\n",
    "model2_coefficients = np.hstack(\n",
    "    [model2_wine.intercept_.reshape(-1, 1), model2_wine.coef_]\n",
    ").T\n",
    "\n",
    "print(\n",
    "    \"The regularization parameter C chosen by this model for each class \"\n",
    "    \"was:\\n\\n\\t{}\\n\\n\"\n",
    "    \"The accuracy scores for this model are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tTEST\\t{:.4f}\\n\"\n",
    "    .format(___, ___, ___)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The coefficients for this model by class are:\"\n",
    "    \"\\n\\n\\t\\t\\t\\tclass\\n\\t\\t\\t\\t0\\t\\t1\\t\\t2\\n\"\n",
    ")\n",
    "\n",
    "coef_names = [\"intercept\"] + list(X_wine_train.columns)\n",
    "\n",
    "for name, values in zip(coef_names, model2_coefficients):\n",
    "    coefs_formatted = [\"{:.4f}\".format(val) for val in values]\n",
    "    print(\"\\t{}   \\t\\t{}\".format(name, \"\\t\\t\".join(coefs_formatted)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
